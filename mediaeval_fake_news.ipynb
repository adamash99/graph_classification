{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python370jvsc74a57bd047b36cd398037ab260698e0cb19f2b90ed6a2036dccab1b3a93a2caa97981f8d",
   "display_name": "Python 3.7.0 64-bit ('env')"
  },
  "metadata": {
   "interpreter": {
    "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediaeval_fake_news.prepare_graphs_basic_info import get_training_dataframe\n",
    "from mediaeval_fake_news.subgraph_sampling import subgraph_random_sampling\n",
    "from mediaeval_fake_news.graph2vec import get_weighted_average_embeddings, get_largest_subgraph_embedding\n",
    "from mediaeval_fake_news.deepwalk_utils import clean_for_deepwalk\n",
    "from mediaeval_fake_news.models.rnn import Rnn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "dataset_path = \"../datasets/mediaeval_fake_news_dataset/graphs/\"\n",
    "df, graph_dict, node_df = get_training_dataframe(dataset_path, two_class=False, get_node_df=True)\n",
    "#node_df = node_df.set_index(['graph_id', 'node_id'])\n",
    "print(graph_dict.keys()) # in this order in the df\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['conspiracy_graphs', 'non_conspiracy_graphs', 'other_conspiracy_graphs'])\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Brainome"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basic features\n",
    "\n",
    "df.shape\n",
    "df.to_csv(\"basic_features.csv\", index=False)\n",
    "# run btc -measureonly basic_features.csv -target label -ignorecolumns nodeID_list\n"
   ]
  },
  {
   "source": [
    "for two class, brainome gives Best guess accuracy: 88.40%. This is just choosing 0 for everything. Got yellow. For two class, also got the same as choosing most common\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "     weighted_g2v0  weighted_g2v1  weighted_g2v2  weighted_g2v3  \\\n0        -0.135185      -0.048401      -0.039733       0.052115   \n1        -0.166506      -0.058801      -0.061478       0.057558   \n2        -0.074059      -0.031809      -0.025210       0.028706   \n3        -0.096855      -0.040148      -0.038347       0.037246   \n4        -0.078578      -0.033788      -0.031015       0.034652   \n..             ...            ...            ...            ...   \n265      -0.119380      -0.044606      -0.038471       0.041389   \n266      -0.102998      -0.034455      -0.030975       0.034713   \n267      -0.147454      -0.052583      -0.057560       0.056190   \n268      -0.005257       0.000199       0.000832       0.001242   \n269      -0.136005      -0.041924      -0.054682       0.067163   \n\n     weighted_g2v4  weighted_g2v5  weighted_g2v6  weighted_g2v7  \\\n0        -0.140880       0.107147      -0.059703      -0.047766   \n1        -0.184593       0.148296      -0.057601      -0.060534   \n2        -0.074920       0.061906      -0.036715      -0.027512   \n3        -0.092192       0.074869      -0.029323      -0.041718   \n4        -0.087756       0.071091      -0.030422      -0.028083   \n..             ...            ...            ...            ...   \n265      -0.121399       0.099895      -0.046840      -0.043110   \n266      -0.103247       0.075003      -0.038463      -0.041934   \n267      -0.171025       0.146895      -0.072731      -0.048643   \n268      -0.004764       0.002313      -0.001831      -0.000076   \n269      -0.122318       0.098035      -0.055701      -0.051978   \n\n     weighted_g2v8  weighted_g2v9  ...  weighted_g2v55  weighted_g2v56  \\\n0         0.025264       0.053888  ...        0.051684        0.111765   \n1         0.020224       0.068468  ...        0.072657        0.124573   \n2         0.012253       0.033411  ...        0.034663        0.066087   \n3         0.010148       0.043166  ...        0.046897        0.082165   \n4         0.016498       0.028220  ...        0.029060        0.067963   \n..             ...            ...  ...             ...             ...   \n265       0.013045       0.045818  ...        0.044321        0.091078   \n266       0.014526       0.033481  ...        0.036479        0.072301   \n267       0.017665       0.060857  ...        0.054055        0.116071   \n268       0.000840       0.001468  ...        0.002257        0.000675   \n269       0.028912       0.066033  ...        0.054393        0.104811   \n\n     weighted_g2v57  weighted_g2v58  weighted_g2v59  weighted_g2v60  \\\n0         -0.000768        0.051560        0.172556       -0.056201   \n1          0.001599        0.080217        0.233222       -0.069000   \n2         -0.002857        0.034992        0.098433       -0.027912   \n3          0.003455        0.043655        0.126909       -0.047726   \n4         -0.002160        0.032116        0.111422       -0.027850   \n..              ...             ...             ...             ...   \n265        0.006572        0.052187        0.156923       -0.046507   \n266        0.004753        0.046622        0.120859       -0.039335   \n267       -0.007857        0.081087        0.213297       -0.068011   \n268       -0.000042        0.001870        0.002764       -0.002260   \n269       -0.007411        0.066787        0.182105       -0.058175   \n\n     weighted_g2v61  weighted_g2v62  weighted_g2v63  label  \n0         -0.206576       -0.131849       -0.032903      1  \n1         -0.245228       -0.159604       -0.037272      1  \n2         -0.122374       -0.080374       -0.010138      1  \n3         -0.144647       -0.093099       -0.017362      1  \n4         -0.119581       -0.078706       -0.010749      1  \n..              ...             ...             ...    ...  \n265       -0.174894       -0.109014       -0.023418      1  \n266       -0.137999       -0.088078       -0.022270      1  \n267       -0.221993       -0.164980       -0.022520      1  \n268       -0.006081       -0.002542        0.001515      1  \n269       -0.184634       -0.114066       -0.024392      1  \n\n[270 rows x 65 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weighted_g2v0</th>\n      <th>weighted_g2v1</th>\n      <th>weighted_g2v2</th>\n      <th>weighted_g2v3</th>\n      <th>weighted_g2v4</th>\n      <th>weighted_g2v5</th>\n      <th>weighted_g2v6</th>\n      <th>weighted_g2v7</th>\n      <th>weighted_g2v8</th>\n      <th>weighted_g2v9</th>\n      <th>...</th>\n      <th>weighted_g2v55</th>\n      <th>weighted_g2v56</th>\n      <th>weighted_g2v57</th>\n      <th>weighted_g2v58</th>\n      <th>weighted_g2v59</th>\n      <th>weighted_g2v60</th>\n      <th>weighted_g2v61</th>\n      <th>weighted_g2v62</th>\n      <th>weighted_g2v63</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.135185</td>\n      <td>-0.048401</td>\n      <td>-0.039733</td>\n      <td>0.052115</td>\n      <td>-0.140880</td>\n      <td>0.107147</td>\n      <td>-0.059703</td>\n      <td>-0.047766</td>\n      <td>0.025264</td>\n      <td>0.053888</td>\n      <td>...</td>\n      <td>0.051684</td>\n      <td>0.111765</td>\n      <td>-0.000768</td>\n      <td>0.051560</td>\n      <td>0.172556</td>\n      <td>-0.056201</td>\n      <td>-0.206576</td>\n      <td>-0.131849</td>\n      <td>-0.032903</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.166506</td>\n      <td>-0.058801</td>\n      <td>-0.061478</td>\n      <td>0.057558</td>\n      <td>-0.184593</td>\n      <td>0.148296</td>\n      <td>-0.057601</td>\n      <td>-0.060534</td>\n      <td>0.020224</td>\n      <td>0.068468</td>\n      <td>...</td>\n      <td>0.072657</td>\n      <td>0.124573</td>\n      <td>0.001599</td>\n      <td>0.080217</td>\n      <td>0.233222</td>\n      <td>-0.069000</td>\n      <td>-0.245228</td>\n      <td>-0.159604</td>\n      <td>-0.037272</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.074059</td>\n      <td>-0.031809</td>\n      <td>-0.025210</td>\n      <td>0.028706</td>\n      <td>-0.074920</td>\n      <td>0.061906</td>\n      <td>-0.036715</td>\n      <td>-0.027512</td>\n      <td>0.012253</td>\n      <td>0.033411</td>\n      <td>...</td>\n      <td>0.034663</td>\n      <td>0.066087</td>\n      <td>-0.002857</td>\n      <td>0.034992</td>\n      <td>0.098433</td>\n      <td>-0.027912</td>\n      <td>-0.122374</td>\n      <td>-0.080374</td>\n      <td>-0.010138</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.096855</td>\n      <td>-0.040148</td>\n      <td>-0.038347</td>\n      <td>0.037246</td>\n      <td>-0.092192</td>\n      <td>0.074869</td>\n      <td>-0.029323</td>\n      <td>-0.041718</td>\n      <td>0.010148</td>\n      <td>0.043166</td>\n      <td>...</td>\n      <td>0.046897</td>\n      <td>0.082165</td>\n      <td>0.003455</td>\n      <td>0.043655</td>\n      <td>0.126909</td>\n      <td>-0.047726</td>\n      <td>-0.144647</td>\n      <td>-0.093099</td>\n      <td>-0.017362</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.078578</td>\n      <td>-0.033788</td>\n      <td>-0.031015</td>\n      <td>0.034652</td>\n      <td>-0.087756</td>\n      <td>0.071091</td>\n      <td>-0.030422</td>\n      <td>-0.028083</td>\n      <td>0.016498</td>\n      <td>0.028220</td>\n      <td>...</td>\n      <td>0.029060</td>\n      <td>0.067963</td>\n      <td>-0.002160</td>\n      <td>0.032116</td>\n      <td>0.111422</td>\n      <td>-0.027850</td>\n      <td>-0.119581</td>\n      <td>-0.078706</td>\n      <td>-0.010749</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>-0.119380</td>\n      <td>-0.044606</td>\n      <td>-0.038471</td>\n      <td>0.041389</td>\n      <td>-0.121399</td>\n      <td>0.099895</td>\n      <td>-0.046840</td>\n      <td>-0.043110</td>\n      <td>0.013045</td>\n      <td>0.045818</td>\n      <td>...</td>\n      <td>0.044321</td>\n      <td>0.091078</td>\n      <td>0.006572</td>\n      <td>0.052187</td>\n      <td>0.156923</td>\n      <td>-0.046507</td>\n      <td>-0.174894</td>\n      <td>-0.109014</td>\n      <td>-0.023418</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>-0.102998</td>\n      <td>-0.034455</td>\n      <td>-0.030975</td>\n      <td>0.034713</td>\n      <td>-0.103247</td>\n      <td>0.075003</td>\n      <td>-0.038463</td>\n      <td>-0.041934</td>\n      <td>0.014526</td>\n      <td>0.033481</td>\n      <td>...</td>\n      <td>0.036479</td>\n      <td>0.072301</td>\n      <td>0.004753</td>\n      <td>0.046622</td>\n      <td>0.120859</td>\n      <td>-0.039335</td>\n      <td>-0.137999</td>\n      <td>-0.088078</td>\n      <td>-0.022270</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>-0.147454</td>\n      <td>-0.052583</td>\n      <td>-0.057560</td>\n      <td>0.056190</td>\n      <td>-0.171025</td>\n      <td>0.146895</td>\n      <td>-0.072731</td>\n      <td>-0.048643</td>\n      <td>0.017665</td>\n      <td>0.060857</td>\n      <td>...</td>\n      <td>0.054055</td>\n      <td>0.116071</td>\n      <td>-0.007857</td>\n      <td>0.081087</td>\n      <td>0.213297</td>\n      <td>-0.068011</td>\n      <td>-0.221993</td>\n      <td>-0.164980</td>\n      <td>-0.022520</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>-0.005257</td>\n      <td>0.000199</td>\n      <td>0.000832</td>\n      <td>0.001242</td>\n      <td>-0.004764</td>\n      <td>0.002313</td>\n      <td>-0.001831</td>\n      <td>-0.000076</td>\n      <td>0.000840</td>\n      <td>0.001468</td>\n      <td>...</td>\n      <td>0.002257</td>\n      <td>0.000675</td>\n      <td>-0.000042</td>\n      <td>0.001870</td>\n      <td>0.002764</td>\n      <td>-0.002260</td>\n      <td>-0.006081</td>\n      <td>-0.002542</td>\n      <td>0.001515</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>-0.136005</td>\n      <td>-0.041924</td>\n      <td>-0.054682</td>\n      <td>0.067163</td>\n      <td>-0.122318</td>\n      <td>0.098035</td>\n      <td>-0.055701</td>\n      <td>-0.051978</td>\n      <td>0.028912</td>\n      <td>0.066033</td>\n      <td>...</td>\n      <td>0.054393</td>\n      <td>0.104811</td>\n      <td>-0.007411</td>\n      <td>0.066787</td>\n      <td>0.182105</td>\n      <td>-0.058175</td>\n      <td>-0.184634</td>\n      <td>-0.114066</td>\n      <td>-0.024392</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>270 rows × 65 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "consp, non_consp, other_consp = graphs['conspiracy_graphs'], graphs['non_conspiracy_graphs'], graphs['other_conspiracy_graphs']\n",
    "g2v1 = get_weighted_average_embeddings(consp, label=1)\n",
    "g2v2 = get_weighted_average_embeddings(non_consp, label=2)\n",
    "g2v3 = get_weighted_average_embeddings(other_consp, label=3)\n",
    "g2v_dfs = [g2v1, g2v2, g2v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_g2v_embeddings = pd.concat(g2v_dfs)\n",
    "all_g2v_embeddings.to_csv(\"g2v_weighted.csv\", index=False)"
   ]
  },
  {
   "source": [
    "# Using only Deepwalk\n",
    "#### run deepwalk --input ../for_deepwalk.edgelist --format edgelist --output OUTFILE_NAME.txt --workers 4 --seed 42 --undirected false\n",
    "### Important: only run this cell if you want to make a new deepwalk embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f84ea7cba90>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "composite_graph = clean_for_deepwalk(dataset_path, directed=True)\n",
    "!deepwalk --input for_deepwalk.edgelist --format edgelist --output march20.txt --workers 4 --seed 42 --undirected false\n"
   ]
  },
  {
   "source": [
    "### Nodes all have unique IDs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NodeView(('32115079', '449009614', '833469625', '499613745', '450210178', '182510449', '74719838'))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "graph_dict['conspiracy_graphs'][0].nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           deepwalk0  deepwalk1  deepwalk2  deepwalk3  deepwalk4  deepwalk5  \\\n",
       "127166025   0.327048  -0.175018   0.224296  -0.304788  -0.222251   0.535962   \n",
       "61721125    0.514014  -0.111906  -0.504094   0.304461   0.119720   0.193295   \n",
       "127471447   0.320193   0.299958   0.126426  -0.828930   0.136684   0.066215   \n",
       "\n",
       "           deepwalk6  deepwalk7  deepwalk8  deepwalk9  ...  deepwalk54  \\\n",
       "127166025  -0.267580   0.059097   0.178404  -0.844267  ...   -0.244400   \n",
       "61721125   -0.145294  -0.607624   0.072884   0.312199  ...    0.537953   \n",
       "127471447   0.340867  -1.203018  -0.158095   0.751009  ...    0.306866   \n",
       "\n",
       "           deepwalk55  deepwalk56  deepwalk57  deepwalk58  deepwalk59  \\\n",
       "127166025   -0.030048   -0.119165    0.218429   -0.567515    0.166872   \n",
       "61721125    -0.267592   -0.209925    0.365644   -0.741665   -0.070278   \n",
       "127471447   -0.620950    1.211668   -0.151608   -0.585739   -0.606053   \n",
       "\n",
       "           deepwalk60  deepwalk61  deepwalk62  deepwalk63  \n",
       "127166025   -0.065369   -0.233754   -0.055578    0.034475  \n",
       "61721125     0.025231   -0.253768    0.119094    0.541069  \n",
       "127471447    0.755599   -0.407707    0.263721   -0.653597  \n",
       "\n",
       "[3 rows x 64 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deepwalk0</th>\n      <th>deepwalk1</th>\n      <th>deepwalk2</th>\n      <th>deepwalk3</th>\n      <th>deepwalk4</th>\n      <th>deepwalk5</th>\n      <th>deepwalk6</th>\n      <th>deepwalk7</th>\n      <th>deepwalk8</th>\n      <th>deepwalk9</th>\n      <th>...</th>\n      <th>deepwalk54</th>\n      <th>deepwalk55</th>\n      <th>deepwalk56</th>\n      <th>deepwalk57</th>\n      <th>deepwalk58</th>\n      <th>deepwalk59</th>\n      <th>deepwalk60</th>\n      <th>deepwalk61</th>\n      <th>deepwalk62</th>\n      <th>deepwalk63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>127166025</th>\n      <td>0.327048</td>\n      <td>-0.175018</td>\n      <td>0.224296</td>\n      <td>-0.304788</td>\n      <td>-0.222251</td>\n      <td>0.535962</td>\n      <td>-0.267580</td>\n      <td>0.059097</td>\n      <td>0.178404</td>\n      <td>-0.844267</td>\n      <td>...</td>\n      <td>-0.244400</td>\n      <td>-0.030048</td>\n      <td>-0.119165</td>\n      <td>0.218429</td>\n      <td>-0.567515</td>\n      <td>0.166872</td>\n      <td>-0.065369</td>\n      <td>-0.233754</td>\n      <td>-0.055578</td>\n      <td>0.034475</td>\n    </tr>\n    <tr>\n      <th>61721125</th>\n      <td>0.514014</td>\n      <td>-0.111906</td>\n      <td>-0.504094</td>\n      <td>0.304461</td>\n      <td>0.119720</td>\n      <td>0.193295</td>\n      <td>-0.145294</td>\n      <td>-0.607624</td>\n      <td>0.072884</td>\n      <td>0.312199</td>\n      <td>...</td>\n      <td>0.537953</td>\n      <td>-0.267592</td>\n      <td>-0.209925</td>\n      <td>0.365644</td>\n      <td>-0.741665</td>\n      <td>-0.070278</td>\n      <td>0.025231</td>\n      <td>-0.253768</td>\n      <td>0.119094</td>\n      <td>0.541069</td>\n    </tr>\n    <tr>\n      <th>127471447</th>\n      <td>0.320193</td>\n      <td>0.299958</td>\n      <td>0.126426</td>\n      <td>-0.828930</td>\n      <td>0.136684</td>\n      <td>0.066215</td>\n      <td>0.340867</td>\n      <td>-1.203018</td>\n      <td>-0.158095</td>\n      <td>0.751009</td>\n      <td>...</td>\n      <td>0.306866</td>\n      <td>-0.620950</td>\n      <td>1.211668</td>\n      <td>-0.151608</td>\n      <td>-0.585739</td>\n      <td>-0.606053</td>\n      <td>0.755599</td>\n      <td>-0.407707</td>\n      <td>0.263721</td>\n      <td>-0.653597</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 64 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "embeddings = pd.read_csv('deepwalk-master/march20.txt', names=['deepwalk' + str(i) for i in range(64)], sep=\" \")\n",
    "#set index (node labels) to string for consistency with NetworkX\n",
    "as_string = embeddings.index.astype('string')\n",
    "embeddings = embeddings.set_index(as_string)\n",
    "embeddings.head(3)"
   ]
  },
  {
   "source": [
    "### Trying out different Recurrent Architectures\n",
    "#### First, we need to create our dataset and dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediaeval_fake_news.dataset_wrapper import GraphDataset, x_transform, y_transform\n",
    "dataset = GraphDataset(df, embeddings, node_df, x_transform, y_transform)\n",
    "train, valid = random_split(dataset, [1861, 466], generator=th.Generator().manual_seed(42))\n",
    "\n",
    "# need to keep batch_size=1 because of the way the RNN part is set up.\n",
    "train_dataloader = DataLoader(train, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid, batch_size=1, shuffle=True)"
   ]
  },
  {
   "source": [
    "#### Next, we create our RNN Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample_x, sample_y = dataset.__getitem__(0)\n",
    "input_size = sample_x.shape[1]\n",
    "output_size = dataset.num_classes()\n",
    "model = Rnn(input_size=input_size, hidden_size=32, output_size=output_size)\n",
    "model.to(device, dtype=th.float32);"
   ]
  },
  {
   "source": [
    "#### We choose an optimizer and loss function.\n",
    "#### Define any training related constants here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCHS=1\n",
    "criterion = th.nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "source": [
    "#### We define a loop for training our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 0. Loss is 0.5966570377349854\n",
      "tensor([[-0.3855, -0.7290,  0.3539]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5967, grad_fn=<NllLossBackward>)\n",
      "Iteration 10. Loss is 7.537882208824158\n",
      "tensor([[-0.4184, -0.7532,  0.3833]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5707, grad_fn=<NllLossBackward>)\n",
      "Iteration 20. Loss is 16.44843155145645\n",
      "tensor([[-0.4298, -0.7633,  0.3973]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5600, grad_fn=<NllLossBackward>)\n",
      "Iteration 30. Loss is 24.85945165157318\n",
      "tensor([[-0.4391, -0.7721,  0.4096]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5509, grad_fn=<NllLossBackward>)\n",
      "Iteration 40. Loss is 36.337219417095184\n",
      "tensor([[-0.4506, -0.7682,  0.4150]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.4121, grad_fn=<NllLossBackward>)\n",
      "Iteration 50. Loss is 45.823095083236694\n",
      "tensor([[-0.4533, -0.7616,  0.4118]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.4134, grad_fn=<NllLossBackward>)\n",
      "Iteration 60. Loss is 54.23547315597534\n",
      "tensor([[ 0.8866, -0.5311,  0.6469]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.9473, grad_fn=<NllLossBackward>)\n",
      "Iteration 70. Loss is 62.06997108459473\n",
      "tensor([[-0.0740, -0.5562,  0.6904]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5614, grad_fn=<NllLossBackward>)\n",
      "Iteration 80. Loss is 69.93564122915268\n",
      "tensor([[-0.4756, -0.7610,  0.4282]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
      "Iteration 90. Loss is 77.35212486982346\n",
      "tensor([[-0.4837, -0.7692,  0.4403]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5279, grad_fn=<NllLossBackward>)\n",
      "Iteration 100. Loss is 88.11567652225494\n",
      "tensor([[-0.4954, -0.7654,  0.4461]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.4649, grad_fn=<NllLossBackward>)\n",
      "Iteration 110. Loss is 96.4590413570404\n",
      "tensor([[-0.4994, -0.7636,  0.4477]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5222, grad_fn=<NllLossBackward>)\n",
      "Iteration 120. Loss is 107.66934478282928\n",
      "tensor([[-0.5042, -0.7699,  0.4562]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.7426, grad_fn=<NllLossBackward>)\n",
      "Iteration 130. Loss is 115.28599482774734\n",
      "tensor([[-0.5168, -0.7575,  0.4557]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5160, grad_fn=<NllLossBackward>)\n",
      "Iteration 140. Loss is 122.60573315620422\n",
      "tensor([[-0.5306, -0.7545,  0.4689]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5082, grad_fn=<NllLossBackward>)\n",
      "Iteration 150. Loss is 131.1148126721382\n",
      "tensor([[-0.5470, -0.7563,  0.4771]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Iteration 160. Loss is 138.78889253735542\n",
      "tensor([[-0.5567, -0.7586,  0.4861]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4950, grad_fn=<NllLossBackward>)\n",
      "Iteration 170. Loss is 147.486680239439\n",
      "tensor([[-0.5646, -0.7622,  0.4947]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.7462, grad_fn=<NllLossBackward>)\n",
      "Iteration 180. Loss is 156.2292935848236\n",
      "tensor([[-0.5712, -0.7586,  0.4967]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4878, grad_fn=<NllLossBackward>)\n",
      "Iteration 190. Loss is 164.47016808390617\n",
      "tensor([[-0.5751, -0.7627,  0.5029]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4839, grad_fn=<NllLossBackward>)\n",
      "Iteration 200. Loss is 171.7338986992836\n",
      "tensor([[-0.5790, -0.7737,  0.5143]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.5701, grad_fn=<NllLossBackward>)\n",
      "Iteration 210. Loss is 181.3936577141285\n",
      "tensor([[-0.5779, -0.7802,  0.5185]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4744, grad_fn=<NllLossBackward>)\n",
      "Iteration 220. Loss is 188.52415853738785\n",
      "tensor([[-0.5831, -0.7807,  0.5236]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4713, grad_fn=<NllLossBackward>)\n",
      "Iteration 230. Loss is 198.11413782835007\n",
      "tensor([[-0.5861, -0.7827,  0.5265]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4692, grad_fn=<NllLossBackward>)\n",
      "Iteration 240. Loss is 203.5677089691162\n",
      "tensor([[ 0.4734, -0.1150,  0.3725]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(1.0007, grad_fn=<NllLossBackward>)\n",
      "Iteration 250. Loss is 210.84985628724098\n",
      "tensor([[-0.6074, -0.8005,  0.5563]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.6146, grad_fn=<NllLossBackward>)\n",
      "Iteration 260. Loss is 219.09283465147018\n",
      "tensor([[-0.6094, -0.8122,  0.5667]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.6210, grad_fn=<NllLossBackward>)\n",
      "Iteration 270. Loss is 227.27559810876846\n",
      "tensor([[-0.6049, -0.8199,  0.5690]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4437, grad_fn=<NllLossBackward>)\n",
      "Iteration 280. Loss is 231.68795523047447\n",
      "tensor([[-0.6099, -0.8272,  0.5783]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4382, grad_fn=<NllLossBackward>)\n",
      "Iteration 290. Loss is 239.66139608621597\n",
      "tensor([[-0.6143, -0.8398,  0.5910]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4310, grad_fn=<NllLossBackward>)\n",
      "Iteration 300. Loss is 248.26187428832054\n",
      "tensor([[-0.6186, -0.8420,  0.5959]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4281, grad_fn=<NllLossBackward>)\n",
      "Iteration 310. Loss is 255.64553180336952\n",
      "tensor([[-0.6312, -0.8376,  0.6022]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4241, grad_fn=<NllLossBackward>)\n",
      "Iteration 320. Loss is 263.7914029657841\n",
      "tensor([[-0.6359, -0.8382,  0.6062]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4218, grad_fn=<NllLossBackward>)\n",
      "Iteration 330. Loss is 270.6872927546501\n",
      "tensor([[-0.6443, -0.8382,  0.6126]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4180, grad_fn=<NllLossBackward>)\n",
      "Iteration 340. Loss is 280.6266346871853\n",
      "tensor([[-0.6249, -0.7163,  0.7034]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8297, grad_fn=<NllLossBackward>)\n",
      "Iteration 350. Loss is 286.0371837615967\n",
      "tensor([[-0.6662, -0.8313,  0.6237]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4112, grad_fn=<NllLossBackward>)\n",
      "Iteration 360. Loss is 296.1922334432602\n",
      "tensor([[-0.7541, -0.4088,  0.7659]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4237, grad_fn=<NllLossBackward>)\n",
      "Iteration 370. Loss is 302.51828917860985\n",
      "tensor([[-0.6857, -0.8231,  0.6319]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4063, grad_fn=<NllLossBackward>)\n",
      "Iteration 380. Loss is 312.26510724425316\n",
      "tensor([[-0.6995, -0.8142,  0.6352]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4041, grad_fn=<NllLossBackward>)\n",
      "Iteration 390. Loss is 320.42489191889763\n",
      "tensor([[-0.5420, -0.6604,  0.8040]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3998, grad_fn=<NllLossBackward>)\n",
      "Iteration 400. Loss is 327.9505424499512\n",
      "tensor([[-0.7078, -0.8147,  0.6416]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4004, grad_fn=<NllLossBackward>)\n",
      "Iteration 410. Loss is 335.4464000463486\n",
      "tensor([[-0.7124, -0.8134,  0.6440]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3991, grad_fn=<NllLossBackward>)\n",
      "Iteration 420. Loss is 341.2075760066509\n",
      "tensor([[-0.7171, -0.8162,  0.6497]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3959, grad_fn=<NllLossBackward>)\n",
      "Iteration 430. Loss is 350.75825223326683\n",
      "tensor([[-0.7215, -0.8249,  0.6596]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.7717, grad_fn=<NllLossBackward>)\n",
      "Iteration 440. Loss is 358.9554094672203\n",
      "tensor([[-0.7145, -0.8302,  0.6584]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3914, grad_fn=<NllLossBackward>)\n",
      "Iteration 450. Loss is 367.57614347338676\n",
      "tensor([[-0.7143, -0.8258,  0.6548]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3933, grad_fn=<NllLossBackward>)\n",
      "Iteration 460. Loss is 378.79775536060333\n",
      "tensor([[-0.7155, -0.8146,  0.6471]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3973, grad_fn=<NllLossBackward>)\n",
      "Iteration 470. Loss is 386.9691182076931\n",
      "tensor([[-0.7217, -0.8094,  0.6476]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3969, grad_fn=<NllLossBackward>)\n",
      "Iteration 480. Loss is 394.4925346672535\n",
      "tensor([[ 0.0156, -0.2619,  0.8020]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5881, grad_fn=<NllLossBackward>)\n",
      "Iteration 490. Loss is 402.9314710497856\n",
      "tensor([[-0.7316, -0.7991,  0.6469]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8431, grad_fn=<NllLossBackward>)\n",
      "Iteration 500. Loss is 409.8515494465828\n",
      "tensor([[ 0.0727, -0.4387,  0.9332]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.5167, grad_fn=<NllLossBackward>)\n",
      "Iteration 510. Loss is 416.40483343601227\n",
      "tensor([[-0.7494, -0.7891,  0.6520]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3940, grad_fn=<NllLossBackward>)\n",
      "Iteration 520. Loss is 423.45437878370285\n",
      "tensor([[-0.7619, -0.7935,  0.6645]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3872, grad_fn=<NllLossBackward>)\n",
      "Iteration 530. Loss is 431.3695932626724\n",
      "tensor([[-0.7748, -0.7861,  0.6681]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3852, grad_fn=<NllLossBackward>)\n",
      "Iteration 540. Loss is 438.1893803179264\n",
      "tensor([[ 0.2051, -0.6040,  1.1190]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.4571, grad_fn=<NllLossBackward>)\n",
      "Iteration 550. Loss is 445.18089178204536\n",
      "tensor([[-0.7990, -0.7797,  0.6803]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3785, grad_fn=<NllLossBackward>)\n",
      "Iteration 560. Loss is 454.85996544361115\n",
      "tensor([[-0.8003, -0.7833,  0.6842]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8440, grad_fn=<NllLossBackward>)\n",
      "Iteration 570. Loss is 464.50142166018486\n",
      "tensor([[-0.8063, -0.7731,  0.6803]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8318, grad_fn=<NllLossBackward>)\n",
      "Iteration 580. Loss is 469.7291227579117\n",
      "tensor([[-0.8185, -0.7665,  0.6836]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3765, grad_fn=<NllLossBackward>)\n",
      "Iteration 590. Loss is 482.01181995868683\n",
      "tensor([[ 0.2035, -0.2296,  0.8119]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.2487, grad_fn=<NllLossBackward>)\n",
      "Iteration 600. Loss is 488.90411165356636\n",
      "tensor([[-0.8197, -0.7631,  0.6817]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3775, grad_fn=<NllLossBackward>)\n",
      "Iteration 610. Loss is 494.12036713957787\n",
      "tensor([[-0.8240, -0.7675,  0.6884]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3740, grad_fn=<NllLossBackward>)\n",
      "Iteration 620. Loss is 500.0217447280884\n",
      "tensor([[-0.8355, -0.7684,  0.6973]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "Iteration 630. Loss is 506.63382133841515\n",
      "tensor([[-0.8495, -0.7696,  0.7083]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3638, grad_fn=<NllLossBackward>)\n",
      "Iteration 640. Loss is 516.9776622951031\n",
      "tensor([[-0.8537, -0.7709,  0.7123]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3617, grad_fn=<NllLossBackward>)\n",
      "Iteration 650. Loss is 525.6574485003948\n",
      "tensor([[-0.8516, -0.7659,  0.7067]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "Iteration 660. Loss is 532.3377637267113\n",
      "tensor([[-0.8514, -0.7687,  0.7089]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3634, grad_fn=<NllLossBackward>)\n",
      "Iteration 670. Loss is 542.3223277032375\n",
      "tensor([[-0.8546, -0.7660,  0.7089]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8383, grad_fn=<NllLossBackward>)\n",
      "Iteration 680. Loss is 550.5983889997005\n",
      "tensor([[-0.8565, -0.7585,  0.7043]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3657, grad_fn=<NllLossBackward>)\n",
      "Iteration 690. Loss is 558.7487081885338\n",
      "tensor([[-0.8607, -0.7604,  0.7087]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9329, grad_fn=<NllLossBackward>)\n",
      "Iteration 700. Loss is 568.9238015115261\n",
      "tensor([[-0.8515, -0.7625,  0.7038]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3660, grad_fn=<NllLossBackward>)\n",
      "Iteration 710. Loss is 578.5408268272877\n",
      "tensor([[-0.8397, -0.7624,  0.6950]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3704, grad_fn=<NllLossBackward>)\n",
      "Iteration 720. Loss is 585.7013966739178\n",
      "tensor([[-0.8406, -0.7525,  0.6878]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3741, grad_fn=<NllLossBackward>)\n",
      "Iteration 730. Loss is 592.3866603374481\n",
      "tensor([[-0.8427, -0.7551,  0.6913]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3723, grad_fn=<NllLossBackward>)\n",
      "Iteration 740. Loss is 598.9987255632877\n",
      "tensor([[-0.8464, -0.7626,  0.7000]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "Iteration 750. Loss is 608.873976200819\n",
      "tensor([[-0.8454, -0.7654,  0.7015]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3671, grad_fn=<NllLossBackward>)\n",
      "Iteration 760. Loss is 615.6289702951908\n",
      "tensor([[-0.8291, -0.7759,  0.6978]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3690, grad_fn=<NllLossBackward>)\n",
      "Iteration 770. Loss is 626.9874443113804\n",
      "tensor([[-0.1498,  0.2230,  0.9355]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.6033, grad_fn=<NllLossBackward>)\n",
      "Iteration 780. Loss is 637.6719033122063\n",
      "tensor([[-0.8335, -0.7587,  0.6872]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8203, grad_fn=<NllLossBackward>)\n",
      "Iteration 790. Loss is 642.9457640945911\n",
      "tensor([[-0.8394, -0.7523,  0.6864]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3748, grad_fn=<NllLossBackward>)\n",
      "Iteration 800. Loss is 648.2081932127476\n",
      "tensor([[-0.8438, -0.7598,  0.6957]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3700, grad_fn=<NllLossBackward>)\n",
      "Iteration 810. Loss is 657.8996319770813\n",
      "tensor([[-0.8538, -0.7605,  0.7034]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8300, grad_fn=<NllLossBackward>)\n",
      "Iteration 820. Loss is 663.2008458077908\n",
      "tensor([[-0.8649, -0.7560,  0.7078]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8276, grad_fn=<NllLossBackward>)\n",
      "Iteration 830. Loss is 674.2728258669376\n",
      "tensor([[-0.8729, -0.7445,  0.7042]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3656, grad_fn=<NllLossBackward>)\n",
      "Iteration 840. Loss is 685.4228260219097\n",
      "tensor([[-0.8809, -0.7316,  0.6994]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.7991, grad_fn=<NllLossBackward>)\n",
      "Iteration 850. Loss is 691.9042053818703\n",
      "tensor([[-0.8822, -0.7185,  0.6897]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3731, grad_fn=<NllLossBackward>)\n",
      "Iteration 860. Loss is 701.2369331121445\n",
      "tensor([[-0.8827, -0.7191,  0.6905]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "Iteration 870. Loss is 706.3755553364754\n",
      "tensor([[-0.8871, -0.7282,  0.7010]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.7965, grad_fn=<NllLossBackward>)\n",
      "Iteration 880. Loss is 717.3691505491734\n",
      "tensor([[-0.8967, -0.7242,  0.7069]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9684, grad_fn=<NllLossBackward>)\n",
      "Iteration 890. Loss is 724.0662039518356\n",
      "tensor([[-0.9016, -0.7145,  0.7002]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "Iteration 900. Loss is 732.33806848526\n",
      "tensor([[-0.7150, -0.7533,  0.9060]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "Iteration 910. Loss is 740.5873444676399\n",
      "tensor([[-0.7192, -0.7486,  0.9052]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3281, grad_fn=<NllLossBackward>)\n",
      "Iteration 920. Loss is 748.9314414262772\n",
      "tensor([[-0.7247, -0.7458,  0.9068]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9588, grad_fn=<NllLossBackward>)\n",
      "Iteration 930. Loss is 759.1215451359749\n",
      "tensor([[-0.7241, -0.7429,  0.9040]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9566, grad_fn=<NllLossBackward>)\n",
      "Iteration 940. Loss is 767.6261537075043\n",
      "tensor([[-0.7227, -0.7434,  0.9034]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9549, grad_fn=<NllLossBackward>)\n",
      "Iteration 950. Loss is 774.2483722269535\n",
      "tensor([[-0.7235, -0.7383,  0.8997]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "Iteration 960. Loss is 779.3601614236832\n",
      "tensor([[-0.7270, -0.7375,  0.9016]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3296, grad_fn=<NllLossBackward>)\n",
      "Iteration 970. Loss is 790.7020011842251\n",
      "tensor([[-0.7269, -0.7379,  0.9016]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9690, grad_fn=<NllLossBackward>)\n",
      "Iteration 980. Loss is 800.5129978656769\n",
      "tensor([[-0.6960, -0.7569,  0.8922]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(1.9222, grad_fn=<NllLossBackward>)\n",
      "Iteration 990. Loss is 808.7168715298176\n",
      "tensor([[-0.7180, -0.7283,  0.8876]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3361, grad_fn=<NllLossBackward>)\n",
      "Iteration 1000. Loss is 818.5367205440998\n",
      "tensor([[-0.7212, -0.7122,  0.8766]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      "Iteration 1010. Loss is 828.3494194746017\n",
      "tensor([[-0.7198, -0.7048,  0.8695]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3445, grad_fn=<NllLossBackward>)\n",
      "Iteration 1020. Loss is 838.0897682011127\n",
      "tensor([[-0.7238, -0.6921,  0.8594]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3487, grad_fn=<NllLossBackward>)\n",
      "Iteration 1030. Loss is 843.2800807058811\n",
      "tensor([[-0.7242, -0.6914,  0.8616]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3481, grad_fn=<NllLossBackward>)\n",
      "Iteration 1040. Loss is 849.8588854372501\n",
      "tensor([[-0.5522, -0.4611,  0.9967]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3683, grad_fn=<NllLossBackward>)\n",
      "Iteration 1050. Loss is 855.091095328331\n",
      "tensor([[-0.7447, -0.6981,  0.8816]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3384, grad_fn=<NllLossBackward>)\n",
      "Iteration 1060. Loss is 863.2744551599026\n",
      "tensor([[-0.5762, -0.5890,  0.9749]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3516, grad_fn=<NllLossBackward>)\n",
      "Iteration 1070. Loss is 868.2204644978046\n",
      "tensor([[-0.7708, -0.6956,  0.8741]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3373, grad_fn=<NllLossBackward>)\n",
      "Iteration 1080. Loss is 874.775810688734\n",
      "tensor([[-0.7675, -0.6976,  0.8974]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3308, grad_fn=<NllLossBackward>)\n",
      "Iteration 1090. Loss is 879.8885368704796\n",
      "tensor([[-0.7797, -0.7023,  0.9098]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9371, grad_fn=<NllLossBackward>)\n",
      "Iteration 1100. Loss is 886.9764703512192\n",
      "tensor([[-0.7827, -0.7056,  0.9147]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      "Iteration 1110. Loss is 896.8347424566746\n",
      "tensor([[-0.7855, -0.7045,  0.9158]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      "Iteration 1120. Loss is 908.3067104816437\n",
      "tensor([[-0.7793, -0.6953,  0.9038]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "Iteration 1130. Loss is 914.8878556787968\n",
      "tensor([[-0.7827, -0.6882,  0.9004]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(2.0124, grad_fn=<NllLossBackward>)\n",
      "Iteration 1140. Loss is 921.4617376923561\n",
      "tensor([[-0.7812, -0.6867,  0.8980]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3304, grad_fn=<NllLossBackward>)\n",
      "Iteration 1150. Loss is 932.8918615579605\n",
      "tensor([[-0.7776, -0.6872,  0.8958]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      "Iteration 1160. Loss is 942.5899720788002\n",
      "tensor([[-0.7751, -0.6763,  0.8850]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "Iteration 1170. Loss is 952.1604402065277\n",
      "tensor([[-0.7218, -0.7213,  0.8620]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9272, grad_fn=<NllLossBackward>)\n",
      "Iteration 1180. Loss is 960.1831091046333\n",
      "tensor([[-0.7847, -0.6519,  0.8715]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3427, grad_fn=<NllLossBackward>)\n",
      "Iteration 1190. Loss is 967.0196133255959\n",
      "tensor([[-0.7922, -0.6437,  0.8700]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3435, grad_fn=<NllLossBackward>)\n",
      "Iteration 1200. Loss is 973.8168051838875\n",
      "tensor([[-0.7909, -0.6481,  0.8727]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "Iteration 1210. Loss is 982.227528989315\n",
      "tensor([[-0.8576, -0.6424,  0.7962]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(2.0104, grad_fn=<NllLossBackward>)\n",
      "Iteration 1220. Loss is 987.0251398384571\n",
      "tensor([[-0.7847, -0.6661,  0.8831]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3372, grad_fn=<NllLossBackward>)\n",
      "Iteration 1230. Loss is 995.018136292696\n",
      "tensor([[-0.7895, -0.6738,  0.8929]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
      "Iteration 1240. Loss is 1003.1994804739952\n",
      "tensor([[-0.7889, -0.6787,  0.8965]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9062, grad_fn=<NllLossBackward>)\n",
      "Iteration 1250. Loss is 1009.7092118263245\n",
      "tensor([[-0.7971, -0.6781,  0.9018]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9084, grad_fn=<NllLossBackward>)\n",
      "Iteration 1260. Loss is 1021.052092820406\n",
      "tensor([[-0.8076, -0.6676,  0.9004]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8971, grad_fn=<NllLossBackward>)\n",
      "Iteration 1270. Loss is 1029.3553857505322\n",
      "tensor([[-0.8038, -0.6600,  0.8915]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8848, grad_fn=<NllLossBackward>)\n",
      "Iteration 1280. Loss is 1034.3866503238678\n",
      "tensor([[-0.8059, -0.6603,  0.8933]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3324, grad_fn=<NllLossBackward>)\n",
      "Iteration 1290. Loss is 1043.2762006521225\n",
      "tensor([[-0.8118, -0.6613,  0.8954]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      "Iteration 1300. Loss is 1049.8469569385052\n",
      "tensor([[-0.8157, -0.6633,  0.9026]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3281, grad_fn=<NllLossBackward>)\n",
      "Iteration 1310. Loss is 1054.7110331952572\n",
      "tensor([[-0.8235, -0.6682,  0.9121]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3237, grad_fn=<NllLossBackward>)\n",
      "Iteration 1320. Loss is 1064.7885440587997\n",
      "tensor([[-0.8413, -0.6623,  0.8994]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3258, grad_fn=<NllLossBackward>)\n",
      "Iteration 1330. Loss is 1074.929349154234\n",
      "tensor([[-0.8240, -0.6605,  0.9062]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "Iteration 1340. Loss is 1084.5982552468777\n",
      "tensor([[-0.8192, -0.6536,  0.8971]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      "Iteration 1350. Loss is 1091.3359011113644\n",
      "tensor([[-0.8154, -0.6578,  0.8978]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "Iteration 1360. Loss is 1098.1713976264\n",
      "tensor([[-0.8078, -0.6644,  0.8978]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "Iteration 1370. Loss is 1106.1580011546612\n",
      "tensor([[-0.8102, -0.6631,  0.8985]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3300, grad_fn=<NllLossBackward>)\n",
      "Iteration 1380. Loss is 1111.218411296606\n",
      "tensor([[-0.8163, -0.6684,  0.9071]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.9014, grad_fn=<NllLossBackward>)\n",
      "Iteration 1390. Loss is 1121.4069789648056\n",
      "tensor([[-0.8203, -0.6657,  0.9077]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(2.0537, grad_fn=<NllLossBackward>)\n",
      "Iteration 1400. Loss is 1129.569558084011\n",
      "tensor([[-0.8156, -0.6638,  0.9028]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8946, grad_fn=<NllLossBackward>)\n",
      "Iteration 1410. Loss is 1137.0187433063984\n",
      "tensor([[-0.8215, -0.6609,  0.9046]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
      "Iteration 1420. Loss is 1145.2429306209087\n",
      "tensor([[-0.8303, -0.6549,  0.8979]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8816, grad_fn=<NllLossBackward>)\n",
      "Iteration 1430. Loss is 1158.0620537400246\n",
      "tensor([[-0.8321, -0.6360,  0.8914]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      "Iteration 1440. Loss is 1161.4283174574375\n",
      "tensor([[-0.8382, -0.6279,  0.8889]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3345, grad_fn=<NllLossBackward>)\n",
      "Iteration 1450. Loss is 1168.0729018449783\n",
      "tensor([[-0.8467, -0.6292,  0.8959]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "Iteration 1460. Loss is 1174.7462035119534\n",
      "tensor([[-0.8494, -0.6365,  0.9039]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3275, grad_fn=<NllLossBackward>)\n",
      "Iteration 1470. Loss is 1181.1262017786503\n",
      "tensor([[-0.8564, -0.6429,  0.9142]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8799, grad_fn=<NllLossBackward>)\n",
      "Iteration 1480. Loss is 1189.2958410978317\n",
      "tensor([[-0.8642, -0.6393,  0.9166]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      "Iteration 1490. Loss is 1200.3127826154232\n",
      "tensor([[-0.8712, -0.6274,  0.9115]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(2.1068, grad_fn=<NllLossBackward>)\n",
      "Iteration 1500. Loss is 1207.962594717741\n",
      "tensor([[-0.8740, -0.6233,  0.9101]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([1])\n",
      "tensor(1.8582, grad_fn=<NllLossBackward>)\n",
      "Iteration 1510. Loss is 1220.7922634780407\n",
      "tensor([[-0.8781, -0.6026,  0.8955]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3317, grad_fn=<NllLossBackward>)\n",
      "Iteration 1520. Loss is 1227.3248736560345\n",
      "tensor([[-0.8776, -0.5917,  0.8857]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "Iteration 1530. Loss is 1235.5814473032951\n",
      "tensor([[-1.0462, -0.3273,  0.8542]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([0])\n",
      "tensor(2.2763, grad_fn=<NllLossBackward>)\n",
      "Iteration 1540. Loss is 1242.173385411501\n",
      "tensor([[-0.8895, -0.5840,  0.8877]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3355, grad_fn=<NllLossBackward>)\n",
      "Iteration 1550. Loss is 1248.5120949447155\n",
      "tensor([[-0.8985, -0.5852,  0.8950]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3321, grad_fn=<NllLossBackward>)\n",
      "Iteration 1560. Loss is 1259.9021740555763\n",
      "tensor([[-0.9033, -0.5804,  0.8942]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([2])\n",
      "tensor(0.3326, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-2084b9164140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/mediaeval_fake_news/dataset_wrapper.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no x transform provided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/mediaeval_fake_news/dataset_wrapper.py\u001b[0m in \u001b[0;36mx_transform\u001b[0;34m(row, embeddings, node_df)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nodeID_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnode_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mgraph_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'graph_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/mediaeval_fake_news/dataset_wrapper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nodeID_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnode_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mgraph_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'graph_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i, (train_x, train_y) in enumerate(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(train_x).unsqueeze(0)\n",
    "\n",
    "        loss = criterion(out, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(F\"Iteration {i}. Loss is {epoch_loss}\")\n",
    "            # print(out)\n",
    "            # print(train_y)\n",
    "            # print(loss)\n",
    "            # print()"
   ]
  },
  {
   "source": [
    "### Validation Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7130574959699086"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "preds = []\n",
    "actuals = []\n",
    "for i, (valid_x, valid_y) in enumerate(train_dataloader):\n",
    "    out = model.forward(train_x).unsqueeze(0)\n",
    "    pred = th.argmax(out).item()\n",
    "    actual = valid_y.item()\n",
    "    \n",
    "    preds.append(pred)\n",
    "    actuals.append(actual)\n",
    "    \n",
    "accuracy = sum([preds[i] == actuals[i] for i in range(len(actuals))]) / len(preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7130574959699086"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "sum([i == 2 for i in actuals]) / len(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this is underfitting - just predicting 1 for everything. I think. Maybe try to "
   ]
  },
  {
   "source": [
    "## using a basic logistic regression model to test what works"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-edd4120a3db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdw_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw_mean'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdw_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_deepwalk_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdw_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/multimedia_research_group/graph_classification/env/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-edd4120a3db3>\u001b[0m in \u001b[0;36mget_deepwalk_mean\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_deepwalk_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nodeID_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnode_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mavg_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-edd4120a3db3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_deepwalk_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nodeID_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnode_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mavg_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dw_feats = df[['label', 'nodeID_list']]\n",
    "def get_deepwalk_mean(row):\n",
    "    graph_nodes = row['nodeID_list']\n",
    "    node_embed = embeddings[[(node in graph_nodes) for node in embeddings.index]]\n",
    "    avg_embed = node_embed.mean(axis=0)\n",
    "    return avg_embed\n",
    "\n",
    "dw_feats[['dw_mean'+str(i) for i in range(64)]] = dw_feats.apply(get_deepwalk_mean, axis=1)\n",
    "dw_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data first\n",
    "dw_feats = dw_feats.drop('nodeID_list', axis=1)\n",
    "X, y = np.array(dw_feats.drop('label', axis=1)), np.ravel(dw_feats[['label']])\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.33811562230497183,\n",
       " 0.4170142973854589,\n",
       " 0.36276128940856917,\n",
       " 0.3848279938846305,\n",
       " 0.3644198947839967]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "scores = []\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    model = LogisticRegression(random_state=42, class_weight='balanced', C=.0001)\n",
    "    from sklearn import svm\n",
    "    model = svm.SVC(kernel='rbf', class_weight=\"balanced\", C=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    score = matthews_corrcoef(y_val, preds)\n",
    "    #print(preds)\n",
    "    scores.append(score)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}